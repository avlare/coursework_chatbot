# -*- coding: utf-8 -*-
"""helsinki.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tw3CAB7c6WLgMqUTD9BZ0CH_hFrdTbVP
"""


from datasets import concatenate_datasets
from datasets import load_dataset
from transformers import MarianMTModel, MarianTokenizer
from transformers import DataCollatorForSeq2Seq
from peft import get_peft_model, LoraConfig

ds = load_dataset("facebook/flores", "eng_Latn-ukr_Cyrl")
print(ds)

combined_dataset = concatenate_datasets([ds["dev"], ds["devtest"]])
combined_dataset[1]

dataset = combined_dataset.train_test_split(test_size=0.2)

dataset.shape

model_name = "Helsinki-NLP/opus-mt-en-uk"
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)

prefix = "translate English to Ukrainian: "


def preprocess_function(examples):
    inputs = examples['sentence_eng_Latn']
    targets = examples['sentence_ukr_Cyrl']
    model_inputs = tokenizer(inputs, max_length=256, truncation=True)
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=256, truncation=True)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    model.resize_token_embeddings(len(tokenizer))

tokenized_dataset = dataset.map(preprocess_function, batched=True)

print(tokenized_dataset.column_names)

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_name)

import peft
peft_config = LoraConfig(task_type="SEQ_2_SEQ_LM",
                        r=32,
                        lora_alpha=64,
                        lora_dropout=0.001,
                        target_modules = 'all-linear')

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments
from transformers import EarlyStoppingCallback

training_args = Seq2SeqTrainingArguments(
    output_dir="/fine_tuned_Helsinki_en_uk",
    per_device_train_batch_size=64,
    per_device_eval_batch_size=64,
    learning_rate=1e-5,
    num_train_epochs=25,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    remove_unused_columns=True,
    optim="adamw_torch",
    report_to="none",
    logging_strategy="epoch",
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],
)

trainer.train()

log_history = trainer.state.log_history

import matplotlib.pyplot as plt

epochs = []
training_loss = []
validation_loss = []

train_losses = {}
val_losses = {}

for entry in log_history:
    epoch = int(entry["epoch"])

    if "loss" in entry:
        train_losses[epoch] = entry["loss"]
    if "eval_loss" in entry:
        val_losses[epoch] = entry["eval_loss"]

epochs = sorted(train_losses.keys())
training_loss = [train_losses[e] for e in epochs]
validation_loss = [val_losses[e] for e in epochs]

plt.figure(figsize=(10, 5))
plt.plot(epochs, training_loss, label="Training Loss", marker='o', linestyle='-', color="blue")
plt.plot(epochs, validation_loss, label="Validation Loss", marker='o', linestyle='--', color="red")

plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training vs Validation Loss")
plt.legend()
plt.grid()
plt.show()

model.save_pretrained("fine_tuned_helsinki")
tokenizer.save_pretrained("fine_tuned_helsinki")
