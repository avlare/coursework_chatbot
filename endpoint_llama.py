# -*- coding: utf-8 -*-
"""endpoint_llama.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cgeMgmUXwxu4fACzUsxv24gu88dYo_7c
"""


# This is example of inference endpoints using Colab.


from fastapi import FastAPI, Request
import nest_asyncio
from pyngrok import ngrok
import uvicorn
import torch
import re
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel


base_model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.2-1B-Instruct",
    device_map="auto",
    torch_dtype=torch.float16
)

model = PeftModel.from_pretrained(base_model, "avlare/llama-therapist-full")

tokenizer = AutoTokenizer.from_pretrained("avlare/llama-therapist-full")

app = FastAPI()

def parse_chat_transcript(text):
    pattern = r'(system|user|assistant)\n\n(.*?)(?=(?:system|user|assistant)\n\n|\Z)'
    matches = re.findall(pattern, text, re.DOTALL)

    mess = [{"role": role, "content": content.strip()} for role, content in matches]
    return mess

@app.post("/generate/")
async def generate_text(request: Request):
    data = await request.json()

    messages = data.get("messages", [])

    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    output = model.generate(**inputs, max_new_tokens=256)

    decoded = tokenizer.decode(output[0], skip_special_tokens=True)
    pattern = r'(system|user|assistant)\n\n(.*?)(?=(?:system|user|assistant)\n\n|\Z)'
    matches = re.findall(pattern, decoded, re.DOTALL)

    mess = [{"role": role, "content": content.strip()} for role, content in matches]

    response_only = mess[-1]['content']

    return {"response": response_only}

nest_asyncio.apply()
public_url = ngrok.connect(8000)
print(f"üåê Public URL: {public_url}")
uvicorn.run(app, host="0.0.0.0", port=8000)